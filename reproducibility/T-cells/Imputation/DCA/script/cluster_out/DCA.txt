dca: Successfully preprocessed 49344 genes and 765 cells.
Model: "model_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
count (InputLayer)              (None, 49344)        0                                            
__________________________________________________________________________________________________
enc0 (Dense)                    (None, 64)           3158080     count[0][0]                      
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 64)           192         enc0[0][0]                       
__________________________________________________________________________________________________
enc0_act (Activation)           (None, 64)           0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32)           96          center[0][0]                     
__________________________________________________________________________________________________
center_act (Activation)         (None, 32)           0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 64)           192         dec1[0][0]                       
__________________________________________________________________________________________________
dec1_act (Activation)           (None, 64)           0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
mean (Dense)                    (None, 49344)        3207360     dec1_act[0][0]                   
__________________________________________________________________________________________________
size_factors (InputLayer)       (None, 1)            0                                            
__________________________________________________________________________________________________
lambda_2 (Lambda)               (None, 49344)        0           mean[0][0]                       
                                                                 size_factors[0][0]               
__________________________________________________________________________________________________
dispersion (Dense)              (None, 49344)        3207360     dec1_act[0][0]                   
__________________________________________________________________________________________________
pi (Dense)                      (None, 49344)        3207360     dec1_act[0][0]                   
__________________________________________________________________________________________________
slice (SliceLayer)              (None, 49344)        0           lambda_2[0][0]                   
                                                                 dispersion[0][0]                 
                                                                 pi[0][0]                         
==================================================================================================
Total params: 12,784,832
Trainable params: 12,784,512
Non-trainable params: 320
__________________________________________________________________________________________________
Train on 688 samples, validate on 77 samples
Epoch 1/300

 32/688 [>.............................] - ETA: 36s - loss: 0.3553
128/688 [====>.........................] - ETA: 8s - loss: 0.3253 
224/688 [========>.....................] - ETA: 3s - loss: 0.3243
320/688 [============>.................] - ETA: 2s - loss: 0.3100
448/688 [==================>...........] - ETA: 1s - loss: 0.3006
576/688 [========================>.....] - ETA: 0s - loss: 0.2870
688/688 [==============================] - 2s 3ms/step - loss: 0.2798 - val_loss: 0.1919
Epoch 2/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1888
128/688 [====>.........................] - ETA: 0s - loss: 0.2025
256/688 [==========>...................] - ETA: 0s - loss: 0.2122
384/688 [===============>..............] - ETA: 0s - loss: 0.2136
512/688 [=====================>........] - ETA: 0s - loss: 0.2218
640/688 [==========================>...] - ETA: 0s - loss: 0.2173
688/688 [==============================] - 0s 564us/step - loss: 0.2180 - val_loss: 0.2364
Epoch 3/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1953
128/688 [====>.........................] - ETA: 0s - loss: 0.2106
256/688 [==========>...................] - ETA: 0s - loss: 0.1917
352/688 [==============>...............] - ETA: 0s - loss: 0.1859
448/688 [==================>...........] - ETA: 0s - loss: 0.1947
544/688 [======================>.......] - ETA: 0s - loss: 0.1925
640/688 [==========================>...] - ETA: 0s - loss: 0.1924
688/688 [==============================] - 0s 567us/step - loss: 0.1900 - val_loss: 0.2293
Epoch 4/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1467
128/688 [====>.........................] - ETA: 0s - loss: 0.1780
224/688 [========>.....................] - ETA: 0s - loss: 0.1715
320/688 [============>.................] - ETA: 0s - loss: 0.1767
416/688 [=================>............] - ETA: 0s - loss: 0.1821
512/688 [=====================>........] - ETA: 0s - loss: 0.1796
608/688 [=========================>....] - ETA: 0s - loss: 0.1795
688/688 [==============================] - 0s 565us/step - loss: 0.1789 - val_loss: 0.2193
Epoch 5/300

 32/688 [>.............................] - ETA: 0s - loss: 0.2188
128/688 [====>.........................] - ETA: 0s - loss: 0.1795
256/688 [==========>...................] - ETA: 0s - loss: 0.1755
352/688 [==============>...............] - ETA: 0s - loss: 0.1744
448/688 [==================>...........] - ETA: 0s - loss: 0.1752
544/688 [======================>.......] - ETA: 0s - loss: 0.1770
640/688 [==========================>...] - ETA: 0s - loss: 0.1751
688/688 [==============================] - 0s 564us/step - loss: 0.1736 - val_loss: 0.2010
Epoch 6/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1845
160/688 [=====>........................] - ETA: 0s - loss: 0.1811
256/688 [==========>...................] - ETA: 0s - loss: 0.1764
384/688 [===============>..............] - ETA: 0s - loss: 0.1680
512/688 [=====================>........] - ETA: 0s - loss: 0.1698
640/688 [==========================>...] - ETA: 0s - loss: 0.1705
688/688 [==============================] - 0s 561us/step - loss: 0.1704 - val_loss: 0.1878
Epoch 7/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1363
128/688 [====>.........................] - ETA: 0s - loss: 0.1506
256/688 [==========>...................] - ETA: 0s - loss: 0.1566
384/688 [===============>..............] - ETA: 0s - loss: 0.1673
512/688 [=====================>........] - ETA: 0s - loss: 0.1647
640/688 [==========================>...] - ETA: 0s - loss: 0.1661
688/688 [==============================] - 0s 562us/step - loss: 0.1688 - val_loss: 0.1751
Epoch 8/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1435
128/688 [====>.........................] - ETA: 0s - loss: 0.1556
224/688 [========>.....................] - ETA: 0s - loss: 0.1618
320/688 [============>.................] - ETA: 0s - loss: 0.1626
448/688 [==================>...........] - ETA: 0s - loss: 0.1637
544/688 [======================>.......] - ETA: 0s - loss: 0.1670
640/688 [==========================>...] - ETA: 0s - loss: 0.1658
688/688 [==============================] - 0s 566us/step - loss: 0.1671 - val_loss: 0.1692
Epoch 9/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1856
160/688 [=====>........................] - ETA: 0s - loss: 0.1707
256/688 [==========>...................] - ETA: 0s - loss: 0.1816
352/688 [==============>...............] - ETA: 0s - loss: 0.1793
448/688 [==================>...........] - ETA: 0s - loss: 0.1712
576/688 [========================>.....] - ETA: 0s - loss: 0.1677
672/688 [============================>.] - ETA: 0s - loss: 0.1672
688/688 [==============================] - 0s 565us/step - loss: 0.1658 - val_loss: 0.1666
Epoch 10/300

 32/688 [>.............................] - ETA: 0s - loss: 0.2219
128/688 [====>.........................] - ETA: 0s - loss: 0.1700
224/688 [========>.....................] - ETA: 0s - loss: 0.1693
320/688 [============>.................] - ETA: 0s - loss: 0.1584
416/688 [=================>............] - ETA: 0s - loss: 0.1568
512/688 [=====================>........] - ETA: 0s - loss: 0.1608
608/688 [=========================>....] - ETA: 0s - loss: 0.1624
688/688 [==============================] - 0s 565us/step - loss: 0.1650 - val_loss: 0.1638
Epoch 11/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1739
128/688 [====>.........................] - ETA: 0s - loss: 0.1622
224/688 [========>.....................] - ETA: 0s - loss: 0.1642
320/688 [============>.................] - ETA: 0s - loss: 0.1624
416/688 [=================>............] - ETA: 0s - loss: 0.1628
512/688 [=====================>........] - ETA: 0s - loss: 0.1627
608/688 [=========================>....] - ETA: 0s - loss: 0.1641
688/688 [==============================] - 0s 565us/step - loss: 0.1637 - val_loss: 0.1625
Epoch 12/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1824
128/688 [====>.........................] - ETA: 0s - loss: 0.1714
224/688 [========>.....................] - ETA: 0s - loss: 0.1605
320/688 [============>.................] - ETA: 0s - loss: 0.1574
448/688 [==================>...........] - ETA: 0s - loss: 0.1650
576/688 [========================>.....] - ETA: 0s - loss: 0.1645
672/688 [============================>.] - ETA: 0s - loss: 0.1632
688/688 [==============================] - 0s 563us/step - loss: 0.1629 - val_loss: 0.1601
Epoch 13/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1938
128/688 [====>.........................] - ETA: 0s - loss: 0.1904
224/688 [========>.....................] - ETA: 0s - loss: 0.1719
320/688 [============>.................] - ETA: 0s - loss: 0.1745
448/688 [==================>...........] - ETA: 0s - loss: 0.1681
576/688 [========================>.....] - ETA: 0s - loss: 0.1664
688/688 [==============================] - 0s 563us/step - loss: 0.1622 - val_loss: 0.1589
Epoch 14/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1438
128/688 [====>.........................] - ETA: 0s - loss: 0.1754
256/688 [==========>...................] - ETA: 0s - loss: 0.1688
384/688 [===============>..............] - ETA: 0s - loss: 0.1590
512/688 [=====================>........] - ETA: 0s - loss: 0.1551
608/688 [=========================>....] - ETA: 0s - loss: 0.1602
688/688 [==============================] - 0s 563us/step - loss: 0.1613 - val_loss: 0.1588
Epoch 15/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1628
128/688 [====>.........................] - ETA: 0s - loss: 0.1670
256/688 [==========>...................] - ETA: 0s - loss: 0.1698
384/688 [===============>..............] - ETA: 0s - loss: 0.1626
512/688 [=====================>........] - ETA: 0s - loss: 0.1613
640/688 [==========================>...] - ETA: 0s - loss: 0.1623
688/688 [==============================] - 0s 562us/step - loss: 0.1607 - val_loss: 0.1581
Epoch 16/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1701
128/688 [====>.........................] - ETA: 0s - loss: 0.1625
256/688 [==========>...................] - ETA: 0s - loss: 0.1654
352/688 [==============>...............] - ETA: 0s - loss: 0.1612
448/688 [==================>...........] - ETA: 0s - loss: 0.1625
576/688 [========================>.....] - ETA: 0s - loss: 0.1618
688/688 [==============================] - 0s 563us/step - loss: 0.1599 - val_loss: 0.1579
Epoch 17/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1438
128/688 [====>.........................] - ETA: 0s - loss: 0.1570
256/688 [==========>...................] - ETA: 0s - loss: 0.1565
352/688 [==============>...............] - ETA: 0s - loss: 0.1598
448/688 [==================>...........] - ETA: 0s - loss: 0.1617
576/688 [========================>.....] - ETA: 0s - loss: 0.1613
688/688 [==============================] - 0s 564us/step - loss: 0.1592 - val_loss: 0.1577
Epoch 18/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1365
128/688 [====>.........................] - ETA: 0s - loss: 0.1526
256/688 [==========>...................] - ETA: 0s - loss: 0.1538
352/688 [==============>...............] - ETA: 0s - loss: 0.1487
448/688 [==================>...........] - ETA: 0s - loss: 0.1532
544/688 [======================>.......] - ETA: 0s - loss: 0.1511
640/688 [==========================>...] - ETA: 0s - loss: 0.1585
688/688 [==============================] - 0s 564us/step - loss: 0.1588 - val_loss: 0.1578
Epoch 19/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1344
128/688 [====>.........................] - ETA: 0s - loss: 0.1511
256/688 [==========>...................] - ETA: 0s - loss: 0.1484
352/688 [==============>...............] - ETA: 0s - loss: 0.1515
448/688 [==================>...........] - ETA: 0s - loss: 0.1513
576/688 [========================>.....] - ETA: 0s - loss: 0.1578
688/688 [==============================] - 0s 562us/step - loss: 0.1581 - val_loss: 0.1576
Epoch 20/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1551
128/688 [====>.........................] - ETA: 0s - loss: 0.1533
256/688 [==========>...................] - ETA: 0s - loss: 0.1633
352/688 [==============>...............] - ETA: 0s - loss: 0.1630
448/688 [==================>...........] - ETA: 0s - loss: 0.1604
576/688 [========================>.....] - ETA: 0s - loss: 0.1614
688/688 [==============================] - 0s 563us/step - loss: 0.1574 - val_loss: 0.1576
Epoch 21/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1354
128/688 [====>.........................] - ETA: 0s - loss: 0.1415
224/688 [========>.....................] - ETA: 0s - loss: 0.1520
320/688 [============>.................] - ETA: 0s - loss: 0.1555
448/688 [==================>...........] - ETA: 0s - loss: 0.1502
544/688 [======================>.......] - ETA: 0s - loss: 0.1525
640/688 [==========================>...] - ETA: 0s - loss: 0.1543
688/688 [==============================] - 0s 564us/step - loss: 0.1569 - val_loss: 0.1575
Epoch 22/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1239
128/688 [====>.........................] - ETA: 0s - loss: 0.1326
256/688 [==========>...................] - ETA: 0s - loss: 0.1501
352/688 [==============>...............] - ETA: 0s - loss: 0.1477
448/688 [==================>...........] - ETA: 0s - loss: 0.1559
544/688 [======================>.......] - ETA: 0s - loss: 0.1588
672/688 [============================>.] - ETA: 0s - loss: 0.1567
688/688 [==============================] - 0s 563us/step - loss: 0.1564 - val_loss: 0.1571
Epoch 23/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1213
128/688 [====>.........................] - ETA: 0s - loss: 0.1599
256/688 [==========>...................] - ETA: 0s - loss: 0.1480
352/688 [==============>...............] - ETA: 0s - loss: 0.1516
448/688 [==================>...........] - ETA: 0s - loss: 0.1543
576/688 [========================>.....] - ETA: 0s - loss: 0.1561
688/688 [==============================] - 0s 562us/step - loss: 0.1559 - val_loss: 0.1576
Epoch 24/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1597
160/688 [=====>........................] - ETA: 0s - loss: 0.1468
256/688 [==========>...................] - ETA: 0s - loss: 0.1521
384/688 [===============>..............] - ETA: 0s - loss: 0.1534
512/688 [=====================>........] - ETA: 0s - loss: 0.1515
640/688 [==========================>...] - ETA: 0s - loss: 0.1566
688/688 [==============================] - 0s 562us/step - loss: 0.1552 - val_loss: 0.1574
Epoch 25/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1252
160/688 [=====>........................] - ETA: 0s - loss: 0.1549
256/688 [==========>...................] - ETA: 0s - loss: 0.1586
352/688 [==============>...............] - ETA: 0s - loss: 0.1574
448/688 [==================>...........] - ETA: 0s - loss: 0.1541
576/688 [========================>.....] - ETA: 0s - loss: 0.1521
688/688 [==============================] - 0s 561us/step - loss: 0.1547 - val_loss: 0.1572
Epoch 26/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1888
128/688 [====>.........................] - ETA: 0s - loss: 0.1692
256/688 [==========>...................] - ETA: 0s - loss: 0.1623
384/688 [===============>..............] - ETA: 0s - loss: 0.1520
512/688 [=====================>........] - ETA: 0s - loss: 0.1585
640/688 [==========================>...] - ETA: 0s - loss: 0.1535
688/688 [==============================] - 0s 562us/step - loss: 0.1544 - val_loss: 0.1574
Epoch 27/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1515
128/688 [====>.........................] - ETA: 0s - loss: 0.1494
256/688 [==========>...................] - ETA: 0s - loss: 0.1496
352/688 [==============>...............] - ETA: 0s - loss: 0.1493
448/688 [==================>...........] - ETA: 0s - loss: 0.1536
576/688 [========================>.....] - ETA: 0s - loss: 0.1514
688/688 [==============================] - 0s 562us/step - loss: 0.1537 - val_loss: 0.1571
Epoch 28/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1409
128/688 [====>.........................] - ETA: 0s - loss: 0.1743
256/688 [==========>...................] - ETA: 0s - loss: 0.1659
384/688 [===============>..............] - ETA: 0s - loss: 0.1533
512/688 [=====================>........] - ETA: 0s - loss: 0.1513
640/688 [==========================>...] - ETA: 0s - loss: 0.1536
688/688 [==============================] - 0s 562us/step - loss: 0.1533 - val_loss: 0.1576
Epoch 29/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1110
128/688 [====>.........................] - ETA: 0s - loss: 0.1197
256/688 [==========>...................] - ETA: 0s - loss: 0.1480
384/688 [===============>..............] - ETA: 0s - loss: 0.1533
480/688 [===================>..........] - ETA: 0s - loss: 0.1532
576/688 [========================>.....] - ETA: 0s - loss: 0.1506
688/688 [==============================] - 0s 563us/step - loss: 0.1528 - val_loss: 0.1576
Epoch 30/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1309
128/688 [====>.........................] - ETA: 0s - loss: 0.1218
256/688 [==========>...................] - ETA: 0s - loss: 0.1484
384/688 [===============>..............] - ETA: 0s - loss: 0.1521
512/688 [=====================>........] - ETA: 0s - loss: 0.1443
640/688 [==========================>...] - ETA: 0s - loss: 0.1500
688/688 [==============================] - 0s 562us/step - loss: 0.1528 - val_loss: 0.1574
Epoch 31/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1375
128/688 [====>.........................] - ETA: 0s - loss: 0.1387
256/688 [==========>...................] - ETA: 0s - loss: 0.1408
384/688 [===============>..............] - ETA: 0s - loss: 0.1442
512/688 [=====================>........] - ETA: 0s - loss: 0.1451
640/688 [==========================>...] - ETA: 0s - loss: 0.1498
688/688 [==============================] - 0s 560us/step - loss: 0.1520 - val_loss: 0.1578
Epoch 32/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1428
160/688 [=====>........................] - ETA: 0s - loss: 0.1533
256/688 [==========>...................] - ETA: 0s - loss: 0.1474
384/688 [===============>..............] - ETA: 0s - loss: 0.1572
512/688 [=====================>........] - ETA: 0s - loss: 0.1578
640/688 [==========================>...] - ETA: 0s - loss: 0.1532
688/688 [==============================] - 0s 563us/step - loss: 0.1518 - val_loss: 0.1576

Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 33/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1337
128/688 [====>.........................] - ETA: 0s - loss: 0.1401
224/688 [========>.....................] - ETA: 0s - loss: 0.1412
320/688 [============>.................] - ETA: 0s - loss: 0.1426
448/688 [==================>...........] - ETA: 0s - loss: 0.1449
544/688 [======================>.......] - ETA: 0s - loss: 0.1468
640/688 [==========================>...] - ETA: 0s - loss: 0.1487
688/688 [==============================] - 0s 567us/step - loss: 0.1505 - val_loss: 0.1577
Epoch 34/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1401
128/688 [====>.........................] - ETA: 0s - loss: 0.1368
224/688 [========>.....................] - ETA: 0s - loss: 0.1447
320/688 [============>.................] - ETA: 0s - loss: 0.1470
448/688 [==================>...........] - ETA: 0s - loss: 0.1441
544/688 [======================>.......] - ETA: 0s - loss: 0.1457
640/688 [==========================>...] - ETA: 0s - loss: 0.1488
688/688 [==============================] - 0s 570us/step - loss: 0.1504 - val_loss: 0.1578
Epoch 35/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1567
128/688 [====>.........................] - ETA: 0s - loss: 0.1541
224/688 [========>.....................] - ETA: 0s - loss: 0.1498
320/688 [============>.................] - ETA: 0s - loss: 0.1543
448/688 [==================>...........] - ETA: 0s - loss: 0.1522
544/688 [======================>.......] - ETA: 0s - loss: 0.1517
640/688 [==========================>...] - ETA: 0s - loss: 0.1479
688/688 [==============================] - 0s 568us/step - loss: 0.1504 - val_loss: 0.1578
Epoch 36/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1886
128/688 [====>.........................] - ETA: 0s - loss: 0.1670
224/688 [========>.....................] - ETA: 0s - loss: 0.1532
320/688 [============>.................] - ETA: 0s - loss: 0.1511
416/688 [=================>............] - ETA: 0s - loss: 0.1505
512/688 [=====================>........] - ETA: 0s - loss: 0.1515
608/688 [=========================>....] - ETA: 0s - loss: 0.1508
688/688 [==============================] - 0s 564us/step - loss: 0.1502 - val_loss: 0.1579
Epoch 37/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1299
128/688 [====>.........................] - ETA: 0s - loss: 0.1349
256/688 [==========>...................] - ETA: 0s - loss: 0.1433
384/688 [===============>..............] - ETA: 0s - loss: 0.1469
480/688 [===================>..........] - ETA: 0s - loss: 0.1489
576/688 [========================>.....] - ETA: 0s - loss: 0.1506
672/688 [============================>.] - ETA: 0s - loss: 0.1507
688/688 [==============================] - 0s 568us/step - loss: 0.1502 - val_loss: 0.1579
Epoch 38/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1751
160/688 [=====>........................] - ETA: 0s - loss: 0.1477
256/688 [==========>...................] - ETA: 0s - loss: 0.1460
352/688 [==============>...............] - ETA: 0s - loss: 0.1456
448/688 [==================>...........] - ETA: 0s - loss: 0.1516
576/688 [========================>.....] - ETA: 0s - loss: 0.1510
688/688 [==============================] - 0s 562us/step - loss: 0.1501 - val_loss: 0.1580
Epoch 39/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1467
128/688 [====>.........................] - ETA: 0s - loss: 0.1667
256/688 [==========>...................] - ETA: 0s - loss: 0.1587
384/688 [===============>..............] - ETA: 0s - loss: 0.1575
512/688 [=====================>........] - ETA: 0s - loss: 0.1545
640/688 [==========================>...] - ETA: 0s - loss: 0.1524
688/688 [==============================] - 0s 563us/step - loss: 0.1500 - val_loss: 0.1580
Epoch 40/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1386
128/688 [====>.........................] - ETA: 0s - loss: 0.1533
256/688 [==========>...................] - ETA: 0s - loss: 0.1473
384/688 [===============>..............] - ETA: 0s - loss: 0.1450
480/688 [===================>..........] - ETA: 0s - loss: 0.1458
576/688 [========================>.....] - ETA: 0s - loss: 0.1487
672/688 [============================>.] - ETA: 0s - loss: 0.1497
688/688 [==============================] - 0s 565us/step - loss: 0.1499 - val_loss: 0.1581
Epoch 41/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1109
128/688 [====>.........................] - ETA: 0s - loss: 0.1343
256/688 [==========>...................] - ETA: 0s - loss: 0.1427
384/688 [===============>..............] - ETA: 0s - loss: 0.1456
480/688 [===================>..........] - ETA: 0s - loss: 0.1479
608/688 [=========================>....] - ETA: 0s - loss: 0.1466
688/688 [==============================] - 0s 565us/step - loss: 0.1502 - val_loss: 0.1580
Epoch 42/300

 32/688 [>.............................] - ETA: 0s - loss: 0.1404
128/688 [====>.........................] - ETA: 0s - loss: 0.1531
256/688 [==========>...................] - ETA: 0s - loss: 0.1458
352/688 [==============>...............] - ETA: 0s - loss: 0.1481
448/688 [==================>...........] - ETA: 0s - loss: 0.1488
544/688 [======================>.......] - ETA: 0s - loss: 0.1464
640/688 [==========================>...] - ETA: 0s - loss: 0.1499
688/688 [==============================] - 0s 564us/step - loss: 0.1500 - val_loss: 0.1581

Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 00042: early stopping
dca: Calculating reconstructions...
dca: Calculating low dimensional representations...
dca: Saving output(s)...
dca: Saving denoised expression...
dca: Saving latent representations...
It took 177 seconds
